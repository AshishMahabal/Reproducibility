{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(name, cnn_layers, classes, epochs = 500, learning_rate = 0.0002, verbose = 1, seed = 0, test_size = 0.2, \n",
    "        data_folder = \"all\", oversampling = 0, undersampling = 0, oversampling_ratio, undersampling_ratio, \n",
    "        update_func = lasagne.updates.adam, objective_l2=0.0025, train_split_eval_size=0.05, output_folder):\n",
    "    \n",
    "    # NOTE: while running the function the current working directory should be ../name(one of the arguments)/code/ \n",
    "    #       and the dmdt processed data should be in ../name(one of the arguments)/data/data_folder(one of the arguments)/\n",
    "    #       containing X_2d.npy which is a 3D matrix containing dmdts with dimensions as (#dmdts, height of dmdt, width of dmdt),\n",
    "    #       X_features.npy which is a 2D matrix with dimensions (#dmdts, #features) and y.npy containing dmdt labels \n",
    "    #       corresponding to X_2D.npy with dimension (#dmdts,)      \n",
    "    \n",
    "    # Arguments:\n",
    "        \n",
    "    # name: denotes the parent directory for which cnn is to be trained eg: ensemble, cnn_with, cnn_without, gdr21, periodic, \n",
    "    #       trans, ptf_classifier\n",
    "    \n",
    "    # cnn_layers: denotes the list of layers making a CNN. Refer: https://lasagne.readthedocs.io/en/latest/modules/layers.html\n",
    "    #             for different layers which can be used\n",
    "    #     eg:         \n",
    "    #        [\n",
    "    #             (InputLayer, {'name':'input', 'shape': (None, X_train.shape[1], X_train.shape[2], X_train.shape[3])}),\n",
    "\n",
    "    #             (Conv2DLayer, {'name':'conv2d1', 'num_filters': 64, 'filter_size': (5, 5), 'pad': 0, 'nonlinearity':rectify}),\n",
    "    #             (MaxPool2DLayer, {'name':'maxpool1','pool_size': (2, 2)}),\n",
    "    #             (DropoutLayer, {'name':'dropout1','p':0.1}),\n",
    "\n",
    "    #             #(Conv2DLayer, {'name':'conv2d2','num_filters': 128, 'filter_size': (5, 5), 'pad': 2, 'nonlinearity':rectify}),\n",
    "    #             #(MaxPool2DLayer, {'pool_size': (2, 2)}),\n",
    "    #             #(DropoutLayer, {'p':0.3}),         \n",
    "\n",
    "    #             #(Conv2DLayer, {'name':'conv2d3','num_filters': 256, 'filter_size': (5, 5), 'pad': 2, 'nonlinearity':rectify}),\n",
    "    #             #(MaxPool2DLayer, {'pool_size': (2, 2)}),\n",
    "    #             #(DropoutLayer, {'p':0.5}),       \n",
    "\n",
    "    #             #(DenseLayer, {'name':'dense1','num_units': 512}),\n",
    "    #             #(DropoutLayer, {'name':'dropout2','p':0.5}),\n",
    "    #             #(DenseLayer, {'name':'dense2','num_units': 512}),\n",
    "\n",
    "    #             (DenseLayer, {'name':'output','num_units': len(list(set(y))), 'nonlinearity': softmax}),\n",
    "    #       ]\n",
    "        \n",
    "    # classes: a list denoting the class numbers to be used for training the CNN eg: for ensemble CNN, \n",
    "    #          classes = [1,2,3,4,5,6,7,9,10,11,13,18]\n",
    "    \n",
    "    # data_folder: refer to the 'name' argument details\n",
    "    \n",
    "    # epochs, update_func, learning_rate, objective_l2, train_split_eval_size, verbose: denotes NeuralNet parameters\n",
    "    \n",
    "    # output_folder: denotes the name of the directory in which the results of trained CNN will be saved, most of the time it \n",
    "    #                will be similar to name argument\n",
    "    \n",
    "    # oversampling, undersampling: equal to 1 for oversampling or undersampling respectively the training data, else 0\n",
    "    \n",
    "    # oversampling_ratio: Refer ratio argument in \n",
    "    #                     http://contrib.scikit-learn.org/imbalanced-learn/stable/generated/imblearn.over_sampling.SMOTE.html\n",
    "    \n",
    "    # undersampling_ratio: Refer ratio argument in \n",
    "    #                      http://contrib.scikit-learn.org/imbalanced-learn/stable/generated/imblearn.under_sampling.RandomUnderSampler.html\n",
    "     \n",
    "        \n",
    "        \n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "\n",
    "    import os\n",
    "    import copy\n",
    "    import generate\n",
    "    import numpy\n",
    "    import theano\n",
    "    import theano.gpuarray\n",
    "    import pygpu\n",
    "    from pygpu import gpuarray\n",
    "    #gpuarray.use(\"gpu\"+str(0))\n",
    "    #import theano.sandbox.cuda\n",
    "    #theano.sandbox.cuda.use(\"gpu\"+str(0))\n",
    "    #theano.gpuarray.use(\"gpu\" + str(0))\n",
    "    theano.gpuarray.use(\"cuda\" + str(0))\n",
    "    import lasagne\n",
    "    from nolearn.lasagne import NeuralNet, objective, TrainSplit, visualize\n",
    "    from lasagne.nonlinearities import softmax, rectify\n",
    "    from lasagne.layers import InputLayer\n",
    "    from lasagne.layers import Conv2DLayer\n",
    "    from lasagne.layers import MaxPool2DLayer\n",
    "    from lasagne.layers import DropoutLayer\n",
    "    from lasagne.layers import DenseLayer\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import preprocessing\n",
    "    from util import plot_confusion, plot_misclassifications\n",
    "    import numpy as np\n",
    "    from six.moves import cPickle\n",
    "    import pickle\n",
    "    from imblearn.over_sampling import SMOTE \n",
    "    from imblearn.under_sampling import TomekLinks, RandomUnderSampler\n",
    "\n",
    "\n",
    "    # parameters\n",
    "    #epochs = 500\n",
    "    #learning_rate = 0.0002\n",
    "    #verbose = 1\n",
    "    #seed = 0\n",
    "    #classes = [5,6]\n",
    "    #what are classes\n",
    "\n",
    "    #test_size = 0.2\n",
    "\n",
    "    # get data and encode labels\n",
    "    #X_2d, X_features, y, indices = generate.get_data(\"all\", classes=classes, shuffle=True, seed=seed)\n",
    "    X_2d, X_features, y, indices = generate.get_data(data_folder, classes=classes, shuffle=True, seed=seed)\n",
    "    ##sm=SMOTE(random_state=seed)\n",
    "    ##(f,g,h)=X_2d.shape\n",
    "    ##X_2d,y=sm.fit_sample(X_2d.reshape(f,g*h),y)\n",
    "    ##X_2d=X_2d.reshape((X_2d.shape[0],g,h))\n",
    "    #print(scenario)\n",
    "    labelencoder = preprocessing.LabelEncoder()\n",
    "    labelencoder.fit(y)\n",
    "    y = labelencoder.transform(y).astype(numpy.int32)\n",
    "    print(\"Total number of instances: \" + str(len(y)))\n",
    "\n",
    "    # split data (train/test)\n",
    "    X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X_2d, y, indices, test_size=test_size, random_state=seed)\n",
    "    if oversampling==1:\n",
    "        #sm=SMOTE(random_state=seed)\n",
    "        sm=SMOTE(random_state=seed, ratio=oversampling_ratio) #ratio={2:1000,4:1000,5:1000})\n",
    "        (f,g,h)=X_train.shape\n",
    "        X_train,y_train=sm.fit_sample(X_train.reshape(f,g*h),y_train)\n",
    "        X_train=X_train.reshape((X_train.shape[0],g,h))\n",
    "    if undersampling==1:\n",
    "        rus=RandomUnderSampler(random_state=seed, ratio=undersampling_ratio) #ratio={0:1000,1:1000,3:1000})\n",
    "        (ff,gg,hh)=X_train.shape\n",
    "        X_train,y_train=rus.fit_sample(X_train.reshape(ff,gg*hh),y_train)\n",
    "        X_train=X_train.reshape((X_train.shape[0],gg,hh))\n",
    "\n",
    "    #sm=SMOTE(random_state=seed)\n",
    "    #X_train,y_train=sm.fit_sample(X_train,y_train)\n",
    "    X_test_plot = copy.deepcopy(X_test)\n",
    "    # why reshaping ?\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1], X_train.shape[2]))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1], X_test.shape[2]))\n",
    "    print(\"Number of training instances: %i\" % len(y_train))\n",
    "    print(\"Number of test instances: %i\" % len(y_test))\n",
    "\n",
    "    layers = cnn_layers\n",
    "#         [\n",
    "#             (InputLayer, {'name':'input', 'shape': (None, X_train.shape[1], X_train.shape[2], X_train.shape[3])}),\n",
    "\n",
    "#             (Conv2DLayer, {'name':'conv2d1', 'num_filters': 64, 'filter_size': (5, 5), 'pad': 0, 'nonlinearity':rectify}),\n",
    "#             (MaxPool2DLayer, {'name':'maxpool1','pool_size': (2, 2)}),\n",
    "#             (DropoutLayer, {'name':'dropout1','p':0.1}),\n",
    "\n",
    "#             #(Conv2DLayer, {'name':'conv2d2','num_filters': 128, 'filter_size': (5, 5), 'pad': 2, 'nonlinearity':rectify}),\n",
    "#             #(MaxPool2DLayer, {'pool_size': (2, 2)}),\n",
    "#             #(DropoutLayer, {'p':0.3}),         \n",
    "\n",
    "#             #(Conv2DLayer, {'name':'conv2d3','num_filters': 256, 'filter_size': (5, 5), 'pad': 2, 'nonlinearity':rectify}),\n",
    "#             #(MaxPool2DLayer, {'pool_size': (2, 2)}),\n",
    "#             #(DropoutLayer, {'p':0.5}),       \n",
    "\n",
    "#             #(DenseLayer, {'name':'dense1','num_units': 512}),\n",
    "#             #(DropoutLayer, {'name':'dropout2','p':0.5}),\n",
    "#             #(DenseLayer, {'name':'dense2','num_units': 512}),\n",
    "\n",
    "#             (DenseLayer, {'name':'output','num_units': len(list(set(y))), 'nonlinearity': softmax}),\n",
    "#         ]\n",
    "\n",
    "    net = NeuralNet(\n",
    "        layers=layers,\n",
    "        #layers=[('input', InputLayer),\n",
    "        #        ('conv2d1', Conv2DLayer),\n",
    "        #        ('maxpool1', MaxPool2DLayer),\n",
    "        #        ('dropout1', DropoutLayer),\n",
    "        #        ('conv2d2', Conv2DLayer),\n",
    "        #\t    ('conv2d3', Conv2DLayer),\n",
    "        #        ('dense1', DenseLayer),\n",
    "        #        ('dropout2', DropoutLayer),\n",
    "        #\t    ('dense2', DenseLayer),\n",
    "        #        ('output', DenseLayer),\n",
    "        #        ],\n",
    "        max_epochs=epochs,\n",
    "        update=update_func,\n",
    "        update_learning_rate=learning_rate,\n",
    "        objective_l2=objective_l2,\n",
    "        train_split=TrainSplit(eval_size=train_split_eval_size),\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    net.fit(X_train, y_train)\n",
    "    preds = net.predict(X_test)\n",
    "    preds_proba = net.predict_proba(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(\"Accuracy: %f\" % acc)\n",
    "\n",
    "    y_test = labelencoder.inverse_transform(y_test)\n",
    "    preds = labelencoder.inverse_transform(preds)\n",
    "\n",
    "    # plot misclassifications\n",
    "    plot_misclassifications(y_test, preds, X_test_plot, indices_test, output_folder+\"/misclassifications\")\n",
    "\n",
    "    # save output\n",
    "    # os.mkdir(\"cnn_cd\")\n",
    "    numpy.save(output_folder+\"/X_test\",X_test)\n",
    "    numpy.save(output_folder+\"/y_test\",y_test)\n",
    "    numpy.save(output_folder+\"/preds_proba\",preds_proba)\n",
    "    numpy.save(output_folder+\"/preds\",preds)\n",
    "    numpy.savetxt(output_folder+\"/y_test_cnn.csv\", y_test, delimiter=\",\", fmt='%.4f')\n",
    "    numpy.savetxt(output_folder+\"/preds_cnn.csv\", preds, delimiter=\",\", fmt='%.4f')\n",
    "    numpy.savetxt(output_folder+\"/preds_proba_cnn.csv\", preds_proba, delimiter=\",\", fmt='%.4f')\n",
    "    plot_confusion(y_test, preds, output_folder+\"/confusion_cnn_hpercent.png\")\n",
    "    plt1=visualize.plot_conv_weights(net.layers_['conv2d1'])\n",
    "    plt1.savefig(output_folder+\"/filters1.png\")\n",
    "    plt1.close()\n",
    "    plt2=visualize.plot_conv_weights(net.layers_['conv2d2'])\n",
    "    plt2.savefig(output_folder+\"/filters2.png\")\n",
    "    plt3=visualize.plot_conv_weights(net.layers_['conv2d3'])\n",
    "    plt3.savefig(output_folder+\"/filters3.png\")\n",
    "    plt3.close()\n",
    "\n",
    "    f=open(output_folder+'/obj.save_cd','wb')\n",
    "    cPickle.dump(net,f,protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "    #f=open('obj.save','rb')\n",
    "    #net=cPickle.load(f)\n",
    "    #f.close()\n",
    "\n",
    "    #print(net)\n",
    "\n",
    "    print(\"F1 Score: \"+str(f1_score(y_test.reshape(y_test.shape[0]),preds.reshape(preds.shape[0]),average=None)))\n",
    "    print(\"Matthews correlation coefficient (MCC): \"+str(matthews_corrcoef(y_test.reshape(y_test.shape[0]),preds.reshape(preds.shape[0]))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
